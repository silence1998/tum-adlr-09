# 22 11 21 Weekly Meeting

## Organizational

## Environment
### Obstacles

### Collision Check
### Reward Func
- a well designed dense reward can speed up the training
- checkpoint wise subsparse works
- [ ] thresholds for waiting and consistency
- [ ] sub action for later on with SAC-X and Meta RL 
- [ ] SUB STUDY: 
  - [ ] sort the obstacles according the distacne and give weights appropriately
    - [ ] take the time to collisions in to account calc depending on our speed  
  - [ ] or take the closest obstacle only
#### Subsparse Rewards
- [ ] target distance checkpoint reward value
  - doesnt really matter as we only give it several times and it cant be exploited
- [ ] waiting reward value
  - can be exploited, so we might have to punish it if it wait too much
- [ ] e.g. moving 10 step in 1 direction reward value
  
## Algorithm
#### SAC
- [ ] SAC-X better for motion planning against normal SAC?
##### JUMP START IDEA (MO)
- [ ] do a short google search for a reasearch based
~~- [ ] does it actually help, can we test it like the following: -> NOT NEEDED SEE BELOW~~
      ~~- [ ] gridsize: 20 -> 400 possible positions -> 160.000 possible paths~~
  ~~- pretrain:SAC_Train episode ratio~~ 
        ~~- 0:200 -> sigma ~ 1~~
        ~~- 50:200 -> sigma ~ 0.6~~
        ~~- 200:200 ->~~ 
        ~~- 200:10 ->~~  
        ~~- 200:1 ->~~ 
        ~~- num episodes not enough regarding the possible paths~~
    ~~- [ ] gridsize: 5 -> 25 possible positions -> 625 possible paths~~
        ~~- we can try if the astar solution for every target&start pair helps overall or not~~
        ~~- pretrain:SAC Train episode ratio~~
- [ ] compute a new astar result for each new environment and add to the replay buffer
- [ ] first with static env and with the "fill the replay buffer with optimal solution" trick
- [ ] redefine the reward function for it
- this is better than a normal dense reward, dense reward gets complex  
- with astar path than sparse reward we can train quite faster
- [ ] considers astar path as several states and give the rewards accordingly
- [ ] technically give the all correct transitions needed in to the replay buffer with the according reward
- [ ] use this at midterm
### Policy
### Networks
#### Inputs
- [ ] random seed where? there is only one in the seed
  - [ ] choose a global seed
  - sometimes the NN implementation doesn't allow it, but the rest of the algo should be with a seed for experiments to be deterministic
  - e.g. environment: obs,    
#### Outputs

## Optimizer
- [ ]  Reparam noise?
    - gives the actor net the ability to explore
- [ ]  Value loss 0.5?
    - might be a trick
    - check stable baselines or the original SAC paper
    - [ ] check SAC Code

### Loss
- [X] negative loss?
  - comes from entropy  
- [ ] plot the sigma for 
- [ ] weights and biases, should be free for student, alternative tensorboard
## Performance
### Google Cloud
- [ ] Visual Introduction

## MILE STONES
- [ ] how long should the milestone report be?
  - 3 pages with figures and references -> on the project page 
### MIDTERM




