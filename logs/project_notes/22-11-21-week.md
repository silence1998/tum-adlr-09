# 22 11 21 Weekly Meeting

## Organizational

## Environment
### Obstacles
- [ ] Felix if we should focus on SAC-X or moving obstacles for midterm (FELIX)
  - we prefer to switch the SAC-X first because SAC doesn't perform well even in static environment
  - [ ] how to train SAC w/o jump start trick better for static environments (FELIX)
### Collision Check
### Reward Func
**after midterm prioritize SAC-X**
- a well-designed dense reward can speed up the training
- checkpoint wise sub sparse works
- [ ] thresholds for waiting and consistency 
- [ ] sub action for later on with SAC-X and Meta RL 
- [ ] SUB STUDY: 
  - [ ] sort the obstacles according the distance and give weights appropriately
    - [ ] take the time to collisions in to account calc depending on our speed  
  - [ ] or take the closest obstacle only
#### Sub sparse Rewards
**after midterm prioritize SAC-X**
- [ ] target distance checkpoint reward _value_
  - doesn't really matter as we only give it several times and it cant be exploited
- [ ] waiting reward _value_
  - can be exploited, so we might have to punish it if it wait too much
- [ ] e.g. moving 10 step in 1 direction reward value
  
## Algorithm
#### SAC-X
- [ ] read paper again
- [ ] extend SAC accordingly in a new folder
#### SAC
- [ ] SAC-X better for motion planning against normal SAC?
##### JUMP START IDEA (MO)
~~- [ ] does it actually help, can we test it like the following: -> NOT NEEDED SEE BELOW~~
      ~~- [ ] gridsize: 20 -> 400 possible positions -> 160.000 possible paths~~
  ~~- pretrain:SAC_Train episode ratio~~ 
        ~~- 0:200 -> sigma ~ 1~~
        ~~- 50:200 -> sigma ~ 0.6~~
        ~~- 200:200 ->~~ 
        ~~- 200:10 ->~~  
        ~~- 200:1 ->~~ 
        ~~- num episodes not enough regarding the possible paths~~
    ~~- [ ] gridsize: 5 -> 25 possible positions -> 625 possible paths~~
        ~~- we can try if the astar solution for every target&start pair helps overall or not~~
        ~~- pretrain:SAC Train episode ratio~~
- [X] compute a new astar result for each new environment and add to the replay buffer
- [X] first with static env and with the "fill the replay buffer with optimal solution" trick
- [X] redefine the reward function for it
- this is better than a normal dense reward, dense reward gets complex  
- with astar path than sparse reward we can train quite faster
- [X] considers astar path as several states and give the rewards accordingly
- [X] technically give the all correct transitions needed in to the replay buffer with the according reward
- [ ] do a short google search for research based version
- [ ] use this at midterm
  - normal SAC doesn't perform well
  - it converges to a large sigma
    - [ ] on how to train the network for it (FELIX)
  - [ ] try SAC-X
### Policy
### Networks
#### Inputs
- [X] set NN seed
  - [X] choose a global seed
    - see model.py torch.manual_seed(3407)
  - sometimes the NN implementation doesn't allow it, but the rest of the algo should be with a seed for experiments to be deterministic
- [ ] set seed for environment e.g. environment: obs,    
  - how to have several environments with 1 seed?
    - where should we define it
#### Outputs

## Optimizer
- [X] Reparam noise?
    - gives the actor net the ability to explore
    - see the code comments and 22-11-14-week for explanations
- [X]  Value loss 0.5?
    - might be a trick
    - check stable baselines or the original SAC paper
    - just an average of both critic nets
    - [X] check SAC Code

### Loss
- [X] negative loss?
  - comes from entropy  
- [ ] plot the sigma for (VOLKAN)
  - we took the average sigma values of the states in a batch
- [ ] weights and biases, should be free for student, alternative tensorboard (VOLKAN)
## Performance
### Google Cloud
- [ ] Visual Introduction

## MILE STONES
- [ ] how long should the milestone report be?
  - 3 pages with figures and references -> on the project page 
### MIDTERM




