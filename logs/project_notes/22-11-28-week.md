# 22 11 21 Weekly Meeting Questions

## Organizational

---

## Environment
- [ ] set seed for environment e.g. environment: obs,    
  - how to have several environments with 1 seed?
    - [X] where should we define it (FELIX)
    - use the "vector env" so that we can create diff envs
    - [ ] or use a list of seeds for env creation, by generating 1 seed and adding (VOLKAN)
### Obstacles
- [X] Felix if we should focus on SAC-X or moving obstacles for midterm (FELIX)
  - we prefer to switch the SAC-X first because SAC doesn't perform well even in static environment -> doesnt matter both should work
  - we should do SAC-X in grid static env first
  - [X] how to train SAC w/o jump start trick better for static environments (FELIX)
    - [ ] sort obstacles depending on distance when give it in the network (VOLKAN)
    - [ ] hindsight replay buffer -> works well for super sparse reward (1 target, -1 crash) (after sorting obstacle and smooting)
      - it relabels trajectories in replay buffer that could be the correct trajectory for an another environment!
      - the agent can act "idle" basically avoid obstacle, and not reach to goal as 
    - for super sparse reward SAC takes a long time to learn 
### Collision Check
### Reward Func
- [ ] print reward values on the grid to check if RL algo does good?
#### Sub sparse Rewards

---

## Algorithm
#### SAC-X
- works on continuous envs min 45.00
- SAC-X is like an improved hindsight replay buffer
#### SAC
- [ ] SAC-X better for motion planning in static env against normal SAC?
- should also work with grid static env
##### JUMP START IDEA - WARMSTART
- [ ] see 22-11-14 (FELIX)
  - [ ] 100 astar - 400 normal training
  - [ ] 
- [ ] use this at midterm
  - normal SAC doesn't perform well
    - but it does train faster w. pretrain 
  - it converges to a large sigma
    - [X] on how to train the network for it (FELIX)
      - [ ] try a lower entropy value (MO)
      - play with the hyperparameter alpha that decays over time
      - see spinning up implementation of alpha decay OR in original SAC for a dynamic adjusting alpha
  - [ ] try SAC-X
    - [ ] try sorting first for several obstacles (VOLKAN)
    
### Policy
### Networks
#### Inputs
- [ ] set NN seed for comparable result of features, then optimize later on the best model with random seeds
#### Outputs
- [ ] filter the output by a moving average filter for better performance with SAC (MO)
  take a window of action history and take the average of the window (to reduce the high frequency actions) e.g. last 4

  - network output with high std -> filter the resulting actions -> then give it to the agent   
  - should reduce the amount of noise
    - a not smooth signal can lead to jittering
  - don't smooth it too much
    - bigger sliding window means smoother the curve for sigma
- [ ] plot both actions out of network and smoothed version to see the diference (MO)
## Optimizer
- [ ] figure out why it sometimes stops at episode 40 (FELIX) 
### Loss

---

## Performance
### Curves
- https://wandb.ai/tum-adlr-09
### Google Cloud
- [X] directly connected to wandb?
  - yes should directly work if it works in our code as well
## MILE STONES
- [ ] do a short google search for research based version for jump start idea -> wait for paper from FELIX  about it
  - (OPTIONAL) Felix found papers for SAC with prefilled buffers with different methods besides A-star 
    - e.g. Tesla stores driving data and they added a regularization to SAC to match that traj which didn't work
    - do mention it in the MILESTONE REPORT
- [ ] SAC-X vs SAC in grid world 
### MIDTERM




